{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjonesfaa/lumsa1/blob/main/Vaccini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IPBEfGcSsyK",
        "outputId": "b1d950e4-6d77-4c52-fa6e-2928423a6942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=139901df7863146b0b2f0faf36adfbdfa80d13b9aee4f5785f5bbf8574660ac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "3jiVCT7jUaux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZX4fnBbKUgv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window #Funzioni per operare meglio sulle righe\n",
        "\n",
        "def primary_vaccine_count(df):\n",
        "    \"\"\"\n",
        "    Per ogni tipologia di vaccino, contare quante nazioni lo hanno somministrato come vaccino principale\n",
        "    \"\"\"\n",
        "    # Creiamo una \"window\" per trovare il numero massimo di vaccinazioni per locazione\n",
        "    window = Window.partitionBy(df['location']).orderBy(df['total_vaccinations'].desc())\n",
        "\n",
        "    # Aggiungiamo una colonna con il max totale di vaccinazioni per paese\n",
        "    df = df.withColumn('max_vaccination', F.max(df['total_vaccinations']).over(window))\n",
        "\n",
        "    # Filtriamo le righe che hanno il total_vaccinations uguale al max_vaccination\n",
        "    primary_vaccines = df.where(df['total_vaccinations'] == df['max_vaccination']).select(df['location'], df['vaccine'])\n",
        "\n",
        "    # Rimuoviamo i duplicati con \"distinct\"\n",
        "    primary_vaccines = primary_vaccines.distinct()\n",
        "\n",
        "    # Contiamo il numero di nazioni per vaccino\n",
        "    primary_vaccines_count = primary_vaccines.groupby('vaccine').agg(F.countDistinct('location').alias('count'))\n",
        "    \n",
        "    return primary_vaccines_count\n",
        "\n",
        "df = spark.read.csv(\"vaccini.csv\", inferSchema=True, header=True)\n",
        "result = primary_vaccine_count(df)\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2DbXP5Qe9mq",
        "outputId": "0bb4e2d6-8b91-4851-aa7e-96c7012397d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|          vaccine|count|\n",
            "+-----------------+-----+\n",
            "|          Sinovac|    3|\n",
            "|          Moderna|    2|\n",
            "|  Pfizer/BioNTech|   36|\n",
            "|Sinopharm/Beijing|    2|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window #Funzioni per operare meglio sulle righe\n",
        "\n",
        "def highest_vaccination_date(df):\n",
        "    \"\"\"\n",
        "    Per ogni nazione identificare la data in cui sono stati somministrati più vaccini (a prescindere dal tipo)\n",
        "    \"\"\"\n",
        "    # Funzione Window per trovare il valore massimo di vaccini somministrati per paese\n",
        "    window = Window.partitionBy(df['location']).orderBy(df['total_vaccinations'].desc())\n",
        "\n",
        "    # Aggiungo una colonna con il valore massimo di vaccini per paese\n",
        "    df = df.withColumn('max_vaccination', F.max(df['total_vaccinations']).over(window))\n",
        "\n",
        "    # Selezioniamo solo le righe dove total_vaccinations e' uguale a max_vaccinations con la propria data\n",
        "    max_vaccination_date = df.where(df['total_vaccinations'] == df['max_vaccination']).select(df['location'], df['date'])\n",
        "    \n",
        "    # Filtriamo i duplicati nel caso ci fossero\n",
        "    max_vaccination_date = max_vaccination_date.distinct()\n",
        "    \n",
        "    return max_vaccination_date\n",
        "\n",
        "df = spark.read.csv(\"vaccini.csv\", inferSchema=True, header=True) #carico il csv vaccini\n",
        "result = highest_vaccination_date(df)\n",
        "# Il risultato finale mostra 2+ date in alcuni casi perche combaciano il numero totale di vaccini somministrati\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMOAHpyRlMLH",
        "outputId": "601b2d53-3616-426b-f153-50092dbb9045"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------------+\n",
            "|      location|               date|\n",
            "+--------------+-------------------+\n",
            "|     Argentina|2022-03-28 00:00:00|\n",
            "|     Argentina|2022-03-29 00:00:00|\n",
            "|       Austria|2022-03-25 00:00:00|\n",
            "|       Belgium|2022-03-18 00:00:00|\n",
            "|       Belgium|2022-03-25 00:00:00|\n",
            "|      Bulgaria|2022-03-18 00:00:00|\n",
            "|         Chile|2022-03-22 00:00:00|\n",
            "|       Croatia|2022-03-25 00:00:00|\n",
            "|        Cyprus|2022-03-18 00:00:00|\n",
            "|       Czechia|2022-03-29 00:00:00|\n",
            "|       Denmark|2022-03-25 00:00:00|\n",
            "|       Ecuador|2022-01-28 00:00:00|\n",
            "|       Estonia|2022-03-18 00:00:00|\n",
            "|European Union|2022-03-29 00:00:00|\n",
            "|       Finland|2022-03-25 00:00:00|\n",
            "|        France|2022-03-28 00:00:00|\n",
            "|       Germany|2022-03-29 00:00:00|\n",
            "|     Hong Kong|2022-03-27 00:00:00|\n",
            "|       Hungary|2022-03-18 00:00:00|\n",
            "|       Iceland|2022-03-28 00:00:00|\n",
            "+--------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}